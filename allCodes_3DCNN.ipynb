**OBJECT TRACKING**

**LUCAS-KANADE** OBJECT TRACKING


import cv2
import os
from datetime import datetime
import numpy as np
from sklearn.metrics import precision_score, recall_score, mean_squared_error
from google.colab.patches import cv2_imshow  # Import cv2_imshow for Colab

def calculate_tracking_metrics(pred, threshold=5):
    precision = np.sum(pred < threshold) / len(pred)
    recall = np.sum(pred < threshold) / len(pred)
    return precision, recall

def calculate_rmse(pred, gt):
    return np.sqrt(mean_squared_error(gt, pred))

def lucas_kanade(video_path, output_folder, output_video_path):
    cap = cv2.VideoCapture(video_path)
    os.makedirs(output_folder, exist_ok=True)

    lk_params = dict(winSize=(15, 15), maxLevel=3, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    frame_count = 0
    ret, old_frame = cap.read()
    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=7)

    frame_height, frame_width, _ = old_frame.shape
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width, frame_height))

    fps_list = []

    while True:
        start_time = datetime.now()

        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.medianBlur(frame, 5)
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if p0 is not None and len(p0) > 0:
            p1, _, _ = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)

            if p1 is not None:
                for i, (new, old) in enumerate(zip(p1, p0)):
                    a, b = new.ravel()
                    frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 255, 0), -1)

                frame_count += 1
                frame_filename = os.path.join(output_folder, f"frame_{frame_count:04d}.png")
                cv2.imwrite(frame_filename, frame)

                out.write(frame)

                cv2_imshow(frame)  # Use cv2_imshow for displaying images in Colab

        k = cv2.waitKey(1) & 0xff
        if k == 27:
            break

        p0 = cv2.goodFeaturesToTrack(frame_gray, mask=None, maxCorners=200, qualityLevel=0.01, minDistance=10, blockSize=7)
        old_gray = frame_gray.copy()

        elapsed_time = (datetime.now() - start_time).total_seconds()
        fps = 1 / elapsed_time
        fps_list.append(fps)
        print(f'FPS: {fps}')

    cap.release()
    out.release()
    cv2.destroyAllWindows()

    average_fps = sum(fps_list) / len(fps_list)

    print(f"Average FPS: {average_fps}")

if __name__ == "__main__":
    input_video_path = "/content/drive/MyDrive/Postal_sign/data_collection/3bharu - signs/BS videos/letter_video001.mp4"
    output_folder = "/content/drive/MyDrive/Sesan-Project01/Object Tracking/Lucas Kanade/KOMAL/letter_Frames"
    output_video_path = "/content/drive/MyDrive/Postal_sign/object_tracking/lucas_kanade/letter/komal.mp4"
    lucas_kanade(input_video_path, output_folder, output_video_path)

import cv2
import os
from datetime import datetime
import numpy as np
from sklearn.metrics import precision_score, recall_score, mean_squared_error
from google.colab.patches import cv2_imshow  # Import cv2_imshow for Colab

def calculate_tracking_metrics(pred, threshold=5):
    precision = np.sum(pred < threshold) / len(pred)
    recall = np.sum(pred < threshold) / len(pred)
    return precision, recall

def calculate_rmse(pred, gt):
    return np.sqrt(mean_squared_error(gt, pred))

def lucas_kanade(video_path, output_folder, output_video_path):
    cap = cv2.VideoCapture(video_path)
    os.makedirs(output_folder, exist_ok=True)

    lk_params = dict(winSize=(15, 15), maxLevel=3, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    frame_count = 0
    ret, old_frame = cap.read()
    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=7)

    frame_height, frame_width, _ = old_frame.shape
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width, frame_height))

    fps_list = []

    while True:
        start_time = datetime.now()

        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.medianBlur(frame, 5)
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if p0 is not None and len(p0) > 0:
            p1, _, _ = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)

            if p1 is not None:
                for i, (new, old) in enumerate(zip(p1, p0)):
                    a, b = new.ravel()
                    frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 255, 0), -1)

                frame_count += 1
                frame_filename = os.path.join(output_folder, f"frame_{frame_count:04d}.png")
                cv2.imwrite(frame_filename, frame)

                out.write(frame)  # Write frame to output video

                cv2_imshow(frame)  # Use cv2_imshow for displaying images in Colab

        k = cv2.waitKey(1) & 0xff
        if k == 27:
            break

        p0 = cv2.goodFeaturesToTrack(frame_gray, mask=None, maxCorners=200, qualityLevel=0.01, minDistance=10, blockSize=7)
        old_gray = frame_gray.copy()

        elapsed_time = (datetime.now() - start_time).total_seconds()
        fps = 1 / elapsed_time
        fps_list.append(fps)
        print(f'FPS: {fps}')

    cap.release()
    out.release()  # Release video writer object
    cv2.destroyAllWindows()

    average_fps = sum(fps_list) / len(fps_list)

    print(f"Average FPS: {average_fps}")

if __name__ == "__main__":
    input_video_path = "/content/drive/MyDrive/Postal_sign/data_collection/1komal_signs/BS_videos/FAX_video002.mp4"
    output_folder = "/content/drive/MyDrive/Postal_sign/object_tracking/lucas_kanade/fax/fax_Frames"
    output_video_path = "/content/drive/MyDrive/Postal_sign/object_tracking/lucas_kanade/fax/komal.mp4"
    lucas_kanade(input_video_path, output_folder, output_video_path)


**Sparse Flow** object tracking

import cv2
import os
from datetime import datetime
import numpy as np
from sklearn.metrics import precision_score, recall_score, mean_squared_error
from google.colab.patches import cv2_imshow  # Import cv2_imshow for Colab

def calculate_tracking_metrics(pred, threshold=5):
    precision = np.sum(pred < threshold) / len(pred)
    recall = np.sum(pred < threshold) / len(pred)
    return precision, recall

def calculate_rmse(pred, gt):
    return np.sqrt(mean_squared_error(gt, pred))

def sparse_optical_flow(video_path, output_folder, output_video_path):
    cap = cv2.VideoCapture(video_path)
    os.makedirs(output_folder, exist_ok=True)

    lk_params = dict(winSize=(15, 15), maxLevel=3, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    frame_count = 0
    ret, old_frame = cap.read()
    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, maxCorners=100, qualityLevel=0.01, minDistance=10, blockSize=7)

    frame_height, frame_width, _ = old_frame.shape
    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width, frame_height))

    fps_list = []

    while True:
        start_time = datetime.now()

        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.medianBlur(frame, 5)
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if p0 is not None and len(p0) > 0:
            p1, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)

            if p1 is not None:
                for i, (new, old) in enumerate(zip(p1, p0)):
                    a, b = new.ravel()
                    frame = cv2.circle(frame, (int(a), int(b)), 5, (0, 255, 0), -1)

                frame_count += 1
                frame_filename = os.path.join(output_folder, f"frame_{frame_count:04d}.png")
                cv2.imwrite(frame_filename, frame)

                out.write(frame)

                cv2_imshow(frame)  # Use cv2_imshow for displaying images in Colab

        k = cv2.waitKey(1) & 0xff
        if k == 27:
            break

        p0 = cv2.goodFeaturesToTrack(frame_gray, mask=None, maxCorners=200, qualityLevel=0.01, minDistance=10, blockSize=7)
        old_gray = frame_gray.copy()

        elapsed_time = (datetime.now() - start_time).total_seconds()
        fps = 1 / elapsed_time
        fps_list.append(fps)
        print(f'FPS: {fps}')

    cap.release()
    out.release()
    cv2.destroyAllWindows()

    average_fps = sum(fps_list) / len(fps_list)

    print(f"Average FPS: {average_fps}")

if __name__ == "__main__":
    input_video_path = "/content/drive/MyDrive/Sesan_Project_02/videos/BS & Original videos/sesan/BS videos/letter_video001.mp4"
    output_folder = "/content/drive/MyDrive/Sesan-Project01/Object Tracking/Sparse Flow/KOMAL/letter/frames"
    output_video_path = "/content/drive/MyDrive/Sesan-Project01/Object Tracking/Sparse Flow/sesan/letter/video.mp4"
    sparse_optical_flow(input_video_path, output_folder, output_video_path)

3DCNN training

# Mount Google Drive to access dataset
from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout

# Function to load and preprocess video data
def load_video_data(directory, categories, frames_per_video=16, image_size=(64, 64)):
    X = []
    y = []

    for category in categories:
        path = os.path.join(directory, category)
        class_label = categories.index(category)

        for video in os.listdir(path):
            video_path = os.path.join(path, video)
            cap = cv2.VideoCapture(video_path)
            frames = []

            for _ in range(frames_per_video):
                ret, frame = cap.read()
                if ret:
                    frame = cv2.resize(frame, image_size)
                    frames.append(frame)
                else:
                    break

            cap.release()

            if len(frames) == frames_per_video:
                X.append(frames)
                y.append(class_label)

    X = np.array(X)
    y = np.array(y)
    return X, y

# Define categories and paths
categories = ["stamp","telegram"]  # Update with your categories
data_directory = '/content/drive/MyDrive/Postal_sign/copy_track_videos'
# Load and preprocess data
X, y = load_video_data(data_directory, categories)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define 3D CNN model
model = Sequential([
    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=X_train.shape[1:]),
    MaxPooling3D(pool_size=(2, 2, 2)),
    Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),
    MaxPooling3D(pool_size=(2, 2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(categories), activation='softmax')
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

3D CNN TRAINING - 13 signs (77 videos) --> model_for_13Signs.h5






# Mount Google Drive to access dataset
from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout

# Function to load and preprocess video data
def load_video_data(directory, categories, frames_per_video=16, image_size=(64, 64)):
    X = []
    y = []

    for category in categories:
        path = os.path.join(directory, category)
        class_label = categories.index(category)

        for video in os.listdir(path):
            video_path = os.path.join(path, video)
            cap = cv2.VideoCapture(video_path)
            frames = []

            for _ in range(frames_per_video):
                ret, frame = cap.read()
                if ret:
                    frame = cv2.resize(frame, image_size)
                    frames.append(frame)
                else:
                    break

            cap.release()

            if len(frames) == frames_per_video:
                X.append(frames)
                y.append(class_label)

    X = np.array(X)
    y = np.array(y)
    return X, y

# Define categories and paths
categories = ["STD","advertisement","airmail","courier","email","envelope","fax","letter","money_order","postcard","stamp","telegram","telephone"]  # Update with your categories
data_directory = '/content/drive/MyDrive/Postal_sign/object_tracking/lucas_kanade'

# Load and preprocess data
X, y = load_video_data(data_directory, categories)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define 3D CNN model
model = Sequential([
    Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=X_train.shape[1:]),
    MaxPooling3D(pool_size=(2, 2, 2)),
    Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),
    MaxPooling3D(pool_size=(2, 2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(categories), activation='softmax')
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Save model
model.save('/content/drive/MyDrive/Postal_sign/3dcnn_model/model_for_13Signs.h5')

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)


3DCNN testing (stamp,telegram)

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Path to the video file you want to predict
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/4hema - signs/Original videos/stamp_original_video001.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)
print("Predicted Label:", predicted_label)

testing for stamp,telegram

import cv2
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('/content/drive/MyDrive/Postal_sign/3dcnn_model/model.h5')

# Define categories
categories = ["stamp", "telegram"]  # Update with your categories

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Path to the video file you want to predict
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/2sesan - signs/Original videos/telegram_original_video001.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)
print("Predicted Label:", predicted_label)


testing for (13 signs)

import cv2
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('/content/drive/MyDrive/Postal_sign/3dcnn_model/model_for_13Signs.h5')

# Define categories
categories = ["STD","advertisement","airmail","courier","email","envelope","fax","letter","money_order","postcard","stamp","telegram","telephone"]  # Update with your categories

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Path to the video file you want to predict
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/1komal_signs/Original videos/TELEGARM_original_video002.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)
print("Predicted Label:", predicted_label)


3DCNN - predicted video frames

import matplotlib.pyplot as plt

# Function to visualize frames with predicted label
def visualize_frames_with_label(video_path, predicted_label):
    cap = cv2.VideoCapture(video_path)
    frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    cap.release()

    # Display frames with predicted label
    plt.figure(figsize=(10, 8))
    for i, frame in enumerate(frames):
        plt.subplot(len(frames)//5 + 1, 5, i+1)
        plt.imshow(frame)
        plt.axis('off')
        plt.title(predicted_label)
        plt.tight_layout()

    plt.show()

# Path to the video file you want to visualize
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/4hema - signs/Original videos/telegram_original_video001.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)

# Visualize the frames with predicted label
visualize_frames_with_label(video_path, predicted_label)

Visualize with video

from IPython.display import display, HTML
from base64 import b64encode

# Function to visualize video with predicted label
def visualize_video_with_label(video_path, predicted_label):
    video_data = open(video_path, "rb").read()
    video_encoded = b64encode(video_data).decode('utf-8')
    video_tag = f'''<video controls width="640" height="480">
                      <source src="data:video/mp4;base64,{video_encoded}" type="video/mp4">
                   </video>'''
    display(HTML(video_tag))
    print("Predicted Label:", predicted_label)

# Path to the video file you want to visualize
video_path = '/content/drive/MyDrive/Postal_sign/copy_track_videos/telegram/sesan.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)

# Visualize the video with predicted label
visualize_video_with_label(video_path, predicted_label)


from IPython.display import display, HTML, Video
import cv2
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('/content/drive/MyDrive/Postal_sign/3dcnn_model/model.h5')

# Define categories
categories = ["stamp", "telegram"]  # Update with your categories

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Function to visualize video with predicted label
def visualize_video_with_label(video_path):
    predicted_label = predict_video_label(video_path)
    display(HTML(f"<h3>Predicted Label: {predicted_label}</h3>"))
    display(Video(video_path))

# Path to the video file you want to visualize
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/1komal_signs/Original videos/STAMP_original_video002.mp4'  # Update with your video path

# Visualize the video with predicted label
visualize_video_with_label(video_path)


visualize the video summary

import cv2
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('/content/drive/MyDrive/Postal_sign/3dcnn_model/model.h5')

# Define categories
categories = ["stamp", "telegram"]  # Update with your categories

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Function to create a video summary with predicted labels
def create_video_summary(video_path, predicted_label):
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    key_frames_indices = np.linspace(0, frame_count - 1, 5, dtype=int)  # Generate 5 key frames

    for i in key_frames_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    cap.release()

    # Display key frames with predicted label
    plt.figure(figsize=(12, 8))
    for i, frame in enumerate(frames):
        plt.subplot(2, 5, i+1)  # Adjust the subplot layout if needed
        plt.imshow(frame)
        plt.axis('off')
        plt.title(predicted_label)
        plt.tight_layout()

    plt.show()


# Path to the video file you want to visualize
video_path = '/content/drive/MyDrive/Postal_sign/data_collection/3bharu - signs/Original videos/telegram_original_video001.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)

# Create the video summary with predicted label
create_video_summary(video_path, predicted_label)


predicted video frames (13 signs)

import cv2
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
model = load_model('/content/drive/MyDrive/Postal_sign/3dcnn_model/model_for_13Signs.h5')

# Define categories
categories = ["STD","advertisement","airmail","courier","email","envelope","fax","letter","money_order","postcard","stamp","telegram","telephone"]  # Update with your categories

# Function to preprocess single video for prediction
def preprocess_video(video_path, frames_per_video=16, image_size=(64, 64)):
    frames = []
    cap = cv2.VideoCapture(video_path)

    for _ in range(frames_per_video):
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, image_size)
            frames.append(frame)
        else:
            break

    cap.release()

    if len(frames) < frames_per_video:
        # Pad frames if video is shorter than frames_per_video
        frames.extend([frames[-1]] * (frames_per_video - len(frames)))

    return np.array(frames)

# Function to predict label of single video
def predict_video_label(video_path):
    preprocessed_video = preprocess_video(video_path)
    preprocessed_video = np.expand_dims(preprocessed_video, axis=0)  # Add batch dimension
    prediction = model.predict(preprocessed_video)
    predicted_label = categories[np.argmax(prediction)]
    return predicted_label

# Function to create a video summary with predicted labels
def create_video_summary(video_path, predicted_label):
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    key_frames_indices = np.linspace(0, frame_count - 1, 5, dtype=int)  # Generate 5 key frames

    for i in key_frames_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    cap.release()

    # Display key frames with predicted label
    plt.figure(figsize=(12, 8))
    for i, frame in enumerate(frames):
        plt.subplot(2, 5, i+1)  # Adjust the subplot layout if needed
        plt.imshow(frame)
        plt.axis('off')
        plt.title(predicted_label)
        plt.tight_layout()

    plt.show()


# Path to the video file you want to visualize
video_path = '/content/drive/MyDrive/Postal_sign/object_tracking/lucas_kanade/courier/sesan.mp4'  # Update with your video path

# Predict label of the video
predicted_label = predict_video_label(video_path)

# Create the video summary with predicted label
create_video_summary(video_path, predicted_label)
